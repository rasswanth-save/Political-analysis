# -*- coding: utf-8 -*-
"""last_step

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15LzWCbYSlmYAs2MywUT4-67926a8ToIH
"""

import pandas as pd
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

# ===== Step 1: Load Data =====
try:
    df = pd.read_csv("tamil_comments_combined_cleaned.csv", encoding="utf-8")
except UnicodeDecodeError:
    print("‚ö†Ô∏è UnicodeDecodeError: Could not decode with utf-8. Trying with latin-1 encoding...")
    try:
        df = pd.read_csv("tamil_comments_combined_cleaned.csv", encoding="latin-1")
    except Exception as e:
        print(f"‚ùå Error reading file with latin-1 encoding: {e}")
        exit()

# ===== Step 2: Verify 'Clean_Comment' column =====
if "Clean_Comment" not in df.columns:
    print("‚ùå Error: 'Clean_Comment' column not found in the DataFrame.")
    exit()

# Keep only non-empty, non-NaN cleaned comments
df_cleaned = df.dropna(subset=["Clean_Comment"]).copy()
df_cleaned = df_cleaned[df_cleaned["Clean_Comment"].str.strip() != ""]

comments = df_cleaned["Clean_Comment"].tolist()
print(f"‚úÖ Loaded {len(comments)} cleaned comments for clustering.")

# ===== Step 3: Load Embedding Model =====
embedding_model = SentenceTransformer("sentence-transformers/paraphrase-xlm-r-multilingual-v1")

# ===== Step 4: Create BERTopic Model =====
# You can set nr_topics=20 for fixed clusters, or "auto" for dynamic reduction
topic_model = BERTopic(embedding_model=embedding_model, language="multilingual", nr_topics=20, calculate_probabilities=True)

# ===== Step 5: Fit Model =====
topics, probs = topic_model.fit_transform(comments)

# ===== Step 6: Inspect Results =====
print("\nüìå Top 10 Topics:")
print(topic_model.get_topic_info().head(10))

# ===== Step 7: Save Output =====
df_cleaned["Cluster"] = topics
df_cleaned.to_csv("youtube_comments_clustered.csv", index=False, encoding="utf-8-sig")
print("\n‚úÖ Clustering complete! Results saved to youtube_comments_clustered.csv")

# ===== Step 8: Visualizations (Interactive) =====
# Install Plotly if needed: pip install plotly
try:
    topic_model.visualize_barchart(top_n_topics=10).show()
    topic_model.visualize_heatmap().show()
    topic_model.visualize_topics().show()
    topic_model.visualize_distribution(probs[0]).show()
except Exception as e:
    print(f"‚ö†Ô∏è Visualization skipped (Plotly issue): {e}")

# ===== Step 9: Print Top Words per Topic =====
def print_top_words_per_topic(topic_model, n_words=10):
    topic_info = topic_model.get_topic_info()  # Get topic summary
    for topic_id in topic_info["Topic"].tolist():
        if topic_id == -1:
            print("\nüü¶ Topic -1 (Outliers)")
        else:
            print(f"\nüü© Topic {topic_id}")

        # Get top words for this topic
        words = topic_model.get_topic(topic_id)
        if words:
            top_words = [w for w, _ in words[:n_words]]
            print("   Top words:", ", ".join(top_words))
        else:
            print("   (No words found)")

print_top_words_per_topic(topic_model, n_words=10)

# ===== Step 10: Show Example Comments per Topic =====
def show_sample_comments(df, n_samples=3):
    grouped = df.groupby("Cluster")
    for cluster_id, group in grouped:
        if cluster_id == -1:
            print("\nüü¶ Topic -1 (Outliers):")
        else:
            print(f"\nüü© Topic {cluster_id} (showing {min(n_samples, len(group))} samples):")

        samples = group["Clean_Comment"].sample(min(n_samples, len(group)), random_state=42)
        for i, comment in enumerate(samples, 1):
            print(f"   {i}. {comment}")

show_sample_comments(df_cleaned, n_samples=3)

!pip install bertopic sentence-transformers umap-learn hdbscan